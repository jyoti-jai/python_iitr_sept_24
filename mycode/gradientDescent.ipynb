{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "50ff93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def differential(fn, input, delta):\n",
    "    output1 = fn(input)\n",
    "    output2 = fn(input+delta)\n",
    "    d = (output2 - output1) / delta\n",
    "    return d\n",
    "\n",
    "def rmse_loss(y_actual, y_pred):\n",
    "    error = np.sqrt(np.mean((y_pred - y_actual) ** 2))\n",
    "    return error\n",
    "\n",
    "# def mse_loss(y_true, y_pred):\n",
    "#     return np.mean((y_true - y_pred) ** 2)\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d57d2503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class LinearRegressionGD:\n",
    "    def __init__(self,learning_rate, n_iterations,loss_fn,\n",
    "                 epsilon= 1e-5):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.n_iterations= n_iterations\n",
    "        self.epsilon = epsilon\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X= np.array(X)\n",
    "        y= np.array(y)\n",
    "        if (X.ndim == 1):\n",
    "            X = X.reshape(-1,1)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.random.randn(n_features,1)\n",
    "        self.bias= np.random.randn()\n",
    "        dw = np.zeros_like (self.weights)\n",
    "        \n",
    "        weights_plus = self.weights.copy()\n",
    "        for n in range(self.n_iterations):\n",
    "            y_pred=(X@self.weights)+self.bias\n",
    "            loss = self.loss_fn(y,y_pred)\n",
    "            print(\"Weights, Bias: \", self.weights, self.bias)\n",
    "            print(\"Loss: \", loss)\n",
    "            for i in range(len(self.weights)):\n",
    "                weights_plus[i] = self.weights[i] + self.epsilon\n",
    "                y_pred_plus = (X@weights_plus)+self.bias\n",
    "                loss_plus = self.loss_fn(y,y_pred_plus)\n",
    "                dw[i] = loss_plus/self.epsilon\n",
    "            bias_plus = self.bias + self.epsilon\n",
    "            y_pred_plus = (X@self.weights)+bias_plus\n",
    "            loss_plus = self.loss_fn(y,y_pred_plus)\n",
    "            db = loss_plus/self.epsilon\n",
    "            self.weights -=self.learning_rate * dw\n",
    "            self.bias -=self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        X=np.array(X)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        return (X@self.weights) + self.bias\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegressionGD2:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, loss_fn=None, epsilon=1e-5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.loss_fn = loss_fn\n",
    "        self.epsilon = epsilon  # small perturbation for numerical gradient\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "            print(X)\n",
    "            print(X.shape)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Randomly initialize weights and bias\n",
    "        self.weights = np.random.randn(n_features)\n",
    "        self.bias = np.random.randn()\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            loss = self.loss_fn(y, y_pred)\n",
    "            print(\"Weights, Bias: \", self.weights, self.bias)\n",
    "            print(\"Loss: \", loss)\n",
    "            \n",
    "            # Approximate gradient for weights\n",
    "            dw = np.zeros_like(self.weights)\n",
    "            for i in range(len(self.weights)):\n",
    "                original_value = self.weights[i]\n",
    "\n",
    "                self.weights[i] = original_value + self.epsilon\n",
    "                y_pred_plus = np.dot(X, self.weights) + self.bias\n",
    "                loss_plus = self.loss_fn(y, y_pred_plus)\n",
    "                \n",
    "\n",
    "                self.weights[i] = original_value - self.epsilon\n",
    "                y_pred_minus = np.dot(X, self.weights) + self.bias\n",
    "                loss_minus = self.loss_fn(y, y_pred_minus)\n",
    "\n",
    "                dw[i] = (loss_plus - loss_minus) / (2 * self.epsilon)\n",
    "\n",
    "                self.weights[i] = original_value  # restore original weight\n",
    "\n",
    "            # Approximate gradient for bias\n",
    "            original_bias = self.bias\n",
    "\n",
    "            self.bias = original_bias + self.epsilon\n",
    "            y_pred_plus = np.dot(X, self.weights) + self.bias\n",
    "            loss_plus = self.loss_fn(y, y_pred_plus)\n",
    "\n",
    "            self.bias = original_bias - self.epsilon\n",
    "            y_pred_minus = np.dot(X, self.weights) + self.bias\n",
    "            loss_minus = self.loss_fn(y, y_pred_minus)\n",
    "\n",
    "            db = (loss_plus - loss_minus) / (2 * self.epsilon)\n",
    "\n",
    "            self.bias = original_bias  # restore original bias\n",
    "\n",
    "            # Gradient descent update\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        return np.dot(X, self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "230ffd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights, Bias:  [[-0.14436676]] -0.4001383519857285\n",
      "Loss:  54.734833850299744\n",
      "Weights, Bias:  [[-54734.56244917]] -54735.0973239789\n",
      "Loss:  53928563395.85464\n",
      "Weights, Bias:  [[-5.39285634e+13]] -53928563446210.83\n",
      "Loss:  5.234921918016733e+28\n",
      "Weights, Bias:  [[-5.23492192e+31]] -5.234921918016733e+31\n",
      "Loss:  4.932793347791758e+64\n",
      "Weights, Bias:  [[-4.93279335e+67]] -4.932793347791758e+67\n",
      "Loss:  4.379841038163351e+136\n",
      "Weights, Bias:  [[-4.37984104e+139]] -4.3798410381633506e+139\n",
      "Loss:  3.4529413535243673e+280\n",
      "Weights, Bias:  [[-3.45294135e+283]] -3.452941353524367e+283\n",
      "Loss:  inf\n",
      "Weights, Bias:  [[-inf]] -inf\n",
      "Loss:  inf\n",
      "Weights, Bias:  [[-inf]] -inf\n",
      "Loss:  inf\n",
      "Weights, Bias:  [[-inf]] -inf\n",
      "Loss:  inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1065005/834518298.py:16: RuntimeWarning: overflow encountered in square\n",
      "  return np.mean((y_true - y_pred) ** 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "model = LinearRegressionGD(learning_rate=0.01, n_iterations=10, \n",
    "                           loss_fn=mse_loss)\n",
    "model.fit(X,y)\n",
    "y_pred= model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5294576a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "[-0.48933722 -0.80459114 -0.21269764]\n",
      "---------\n",
      "[ -2.73661242  -7.25649041 -11.7763684  -16.2962464 ]\n",
      "[  0.26338758  -4.25649041  -8.7763684  -13.2962464 ]\n"
     ]
    }
   ],
   "source": [
    "x = [[1,2,3],[4,5,6],[7,8,9], [10,11,12]]\n",
    "X = np.array(x)\n",
    "w= np.random.randn(3)\n",
    "\n",
    "print(X)\n",
    "print(w)\n",
    "print(\"---------\")\n",
    "y1= X@w \n",
    "y2 = X@w + 3\n",
    "print(y1)\n",
    "print(y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "df76eedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n",
      "[[1.98508459]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "X= np.array(X)\n",
    "# print(X.ndim)\n",
    "# if X.ndim == 1:\n",
    "#     X.reshape(-1,1)\n",
    "#     print(X)\n",
    "#     print(X.shape)\n",
    "\n",
    "\n",
    "if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "        # print(X)\n",
    "        # print(X.shape)\n",
    "    \n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "weights = np.random.randn(n_features,1)\n",
    "bias= np.random.randn()\n",
    "dw = np.zeros_like (weights)\n",
    "weights_plus = weights.copy()\n",
    "# print(weights)\n",
    "# print(weights_plus)\n",
    "weights_plus[0] +=1\n",
    "print(dw)\n",
    "print(weights)\n",
    "# model = LinearRegressionGD(learning_rate=0.01, number_iterations=100, \n",
    "#                            loss_fn=MSE_lossfunction)\n",
    "# model.fit(X,y)\n",
    "# y_pred= model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "model = LinearRegressionGD(learning_rate=0.01, number_iterations=100, \n",
    "                           loss_fn=MSE_lossfunction)\n",
    "model.fit(X,y)\n",
    "y_pred= model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "46339a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array1: [1 2 3 4]\n",
      "array2: [100   2   3   4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "array1 = np.array([1, 2, 3, 4])\n",
    "array2 = array1.copy()  # Creates an independent copy\n",
    "\n",
    "array2[0] = 100  # Modify array2\n",
    "print(\"array1:\", array1)  # Original remains unchanged\n",
    "print(\"array2:\", array2)  # Modified copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae94130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cw_env)",
   "language": "python",
   "name": "cw_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
